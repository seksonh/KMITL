{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seksonh/KMITL/blob/main/sample_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REr1kuYd-JH3"
      },
      "source": [
        "# `wongnai-corpus` Classification Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss45oAVX-JH4"
      },
      "source": [
        "We provide two benchmarks for 5-star multi-class classification of [wongnai-corpus](https://github.com/wongnai/wongnai-corpus): [fastText](https://github.com/facebookresearch/fastText) and [ULMFit](https://github.com/cstorm125/thai2fit). In both cases, we first finetune the embeddings using all data. The benchmark numbers are based on the test set. Performance metric is the micro-averaged F1 by the test set of [Wongnai Challenge](https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction/data).\n",
        "\n",
        "| Model     | Public Micro-F1 | Private Micro-F1 | \n",
        "|-----------|-----------------|------------------|\n",
        "| [**ULMFit Knight**](https://www.facebook.com/photo.php?fbid=10215789035573261&set=pcb.795048317543327&type=3&theater&ifg=1) | **0.61109** | **0.62580** |\n",
        "| [ULMFit](https://github.com/cstorm125/thai2fit/) | 0.59313          | 0.60322           |\n",
        "| fastText | 0.5145          | 0.5109           |\n",
        "| LinearSVC | 0.5022          | 0.4976           |\n",
        "| Kaggle Score | 0.59139          | 0.58139          |\n",
        "| [BERT](https://github.com/ThAIKeras/bert) | 0.56612 | 0.57057 |\n",
        "| [USE](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) | 0.42688 | 0.41031 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3tjytfj-JH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019816b0-907c-45d7-d390-dd1af01d4c85"
      },
      "source": [
        "#uncomment if you are running from google colab\n",
        "!pip install sklearn_crfsuite\n",
        "!pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
        "!pip install fastai==1.0.45\n",
        "!pip install tensorflow_text\n",
        "!wget https://github.com/wongnai/wongnai-corpus/raw/master/review/review_dataset.zip; unzip review_dataset.zip\n",
        "!mkdir wongnai_data; ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.9/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (0.9.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
            "  Using cached https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fastai==1.0.45 (from versions: 0.6, 0.7.0, 1.0.0b7, 1.0.0b8, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.9, 1.0.10, 1.0.11, 1.0.12, 1.0.13, 1.0.14, 1.0.15, 1.0.16, 1.0.17, 1.0.18, 1.0.19, 1.0.20, 1.0.21, 1.0.22, 1.0.24, 1.0.25, 1.0.26, 1.0.27, 1.0.28, 1.0.29, 1.0.30, 1.0.31, 1.0.32, 1.0.33, 1.0.34, 1.0.35, 1.0.36, 1.0.36.post1, 1.0.37, 1.0.38, 1.0.39, 1.0.40, 1.0.41, 1.0.42, 1.0.43.post1, 1.0.44, 1.0.46, 1.0.47, 1.0.47.post1, 1.0.48, 1.0.49, 1.0.50, 1.0.50.post1, 1.0.51, 1.0.52, 1.0.53, 1.0.53.post1, 1.0.53.post2, 1.0.53.post3, 1.0.54, 1.0.55, 1.0.57, 1.0.58, 1.0.59, 1.0.60, 1.0.61, 2.0.0, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.10, 2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 2.0.16, 2.0.17, 2.0.18, 2.0.19, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.1.5, 2.1.6, 2.1.7, 2.1.8, 2.1.9, 2.1.10, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.2.7, 2.3.0, 2.3.1, 2.4, 2.4.1, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.5.4, 2.5.5, 2.5.6, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.7.5, 2.7.6, 2.7.7, 2.7.8, 2.7.9, 2.7.10, 2.7.11)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for fastai==1.0.45\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_text) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (4.22.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.51.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.31.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (67.6.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (15.0.6.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.10.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.2.2)\n",
            "--2023-03-25 15:14:11--  https://github.com/wongnai/wongnai-corpus/raw/master/review/review_dataset.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/wongnai/wongnai-corpus/master/review/review_dataset.zip [following]\n",
            "--2023-03-25 15:14:11--  https://raw.githubusercontent.com/wongnai/wongnai-corpus/master/review/review_dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14338706 (14M) [application/zip]\n",
            "Saving to: ‘review_dataset.zip.2’\n",
            "\n",
            "review_dataset.zip. 100%[===================>]  13.67M  67.5MB/s    in 0.2s    \n",
            "\n",
            "2023-03-25 15:14:12 (67.5 MB/s) - ‘review_dataset.zip.2’ saved [14338706/14338706]\n",
            "\n",
            "Archive:  review_dataset.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==2.0.15\n",
        "!pip install fastai2==0.0.30\n",
        "!pip install fastcore==1.0.16."
      ],
      "metadata": {
        "id": "frE-TKagQM4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srx7CmDT-JIB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "#viz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import CSVLogger\n",
        "\n",
        "from pythainlp import word_tokenize\n",
        "\n",
        "ft_data = 'ft_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFQe33_e-JIF"
      },
      "source": [
        "## Oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGdERnuT-JIF"
      },
      "source": [
        "We oversampled class `1` and `2` for 11 and 3 times respectively in order to balance the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjKq3yXk-JIG"
      },
      "source": [
        "train_df = pd.read_csv('w_review_train.csv',sep=';',header=None).drop_duplicates()\n",
        "train_df.columns = ['review','rating']\n",
        "test_df = pd.read_csv('test_file.csv',sep=';')\n",
        "test_df['rating'] = 0\n",
        "all_df = pd.concat([pd.DataFrame(test_df['review']),\\\n",
        "                   pd.DataFrame(train_df['review'])]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgbKYnFN-JIL"
      },
      "source": [
        "train_df.rating.value_counts() / train_df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14_0fbbT-JIR"
      },
      "source": [
        "two_df = pd.concat([train_df[train_df.rating==2].copy() for i in range(2)]).reset_index(drop=True)\n",
        "one_df = pd.concat([train_df[train_df.rating==1].copy() for i in range(10)]).reset_index(drop=True)\n",
        "train_bal = pd.concat([train_df,one_df,two_df]).reset_index(drop=True)\n",
        "train_bal.rating.value_counts() / train_bal.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f32qxMv-JIU"
      },
      "source": [
        "## [fastText](https://github.com/facebookresearch/fastText) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-S57Uc8-JIV"
      },
      "source": [
        "We used embeddings pretrained on [Thai Wikipedia Dump](https://github.com/facebookresearch/fastText/blob/master/docs/pretrained-vectors.md) and finetuned them using all of `wisesight-sentiment` using skipgram model. After that, we do a 5-class classification.\n",
        "\n",
        "| model    | micro_f1_public | micro_f1_private |\n",
        "|----------|-----------------|------------------|\n",
        "| fastText | 0.5145          | 0.5109           |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9F9oWhm-JIW"
      },
      "source": [
        "df_txts = ['train','train_bal','test']\n",
        "dfs = [train_df,train_bal,test_df]\n",
        "\n",
        "for i in range(3):\n",
        "    df = dfs[i]\n",
        "    ft_lines = []\n",
        "    for _,row in df.iterrows():\n",
        "        ft_lab = f'__label__{row[\"rating\"]}'\n",
        "        ft_text = replace_newline(f'{row[\"review\"]}')\n",
        "        ft_line = f'{ft_lab} {ft_text}'\n",
        "        ft_lines.append(ft_line)\n",
        "\n",
        "    doc = '\\n'.join(ft_lines)\n",
        "    with open(f'{ft_data}{df_txts[i]}.txt','w') as f:\n",
        "        f.write(doc)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohZTyhU_-JIa"
      },
      "source": [
        "#for fasttext embedding finetuning\n",
        "ft_lines = []\n",
        "for _,row in all_df.iterrows():\n",
        "    ft_lab = '__label__0'\n",
        "    ft_text = replace_newline(f'{row[\"review\"]}')\n",
        "    ft_line = f'{ft_lab} {ft_text}'\n",
        "    ft_lines.append(ft_line)\n",
        "\n",
        "doc = '\\n'.join(ft_lines)\n",
        "with open(f'{ft_data}df_all.txt','w') as f:\n",
        "    f.write(doc)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SscxRB86-JIe"
      },
      "source": [
        "#finetune with all data\n",
        "!/home/charin/fastText-0.1.0/fasttext skipgram \\\n",
        "-pretrainedVectors 'model/wiki.th.vec' -dim 300 \\\n",
        "-input ft_data/df_all.txt -output 'model/finetuned'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjXgdLSF-JIi"
      },
      "source": [
        "#train classifier\n",
        "!/home/charin/fastText-0.1.0/fasttext supervised \\\n",
        "-input 'ft_data/train_bal.txt' -output 'model/wongnai_bal' \\\n",
        "-pretrainedVectors 'model/finetuned.vec' -epoch 5 -dim 300 -wordNgrams 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhgyadWh-JIl"
      },
      "source": [
        "#get prediction\n",
        "preds = !/home/charin/fastText-0.1.0/fasttext predict 'model/wongnai_bal.bin' 'ft_data/test.txt'\n",
        "pred_lab = np.array([int(i.split('__')[-1]) for i in preds])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012rurfi-JIp"
      },
      "source": [
        "submit_df = pd.DataFrame({'reviewID':test_df.reviewID,\n",
        "                          'rating':pred_lab})\n",
        "submit_df.head()\n",
        "submit_df.to_csv('submit_fastttext_bal.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg4tAHpR-JIs"
      },
      "source": [
        "## LinearSVC Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcRgzg2W-JIs"
      },
      "source": [
        "Code for LinearSVC is provided by [@lukkiddd](https://github.com/lukkiddd).\n",
        "\n",
        "| model     | micro_f1_public | micro_f1_private | \n",
        "|-----------|-----------------|------------------|\n",
        "| LinearSVC | 0.5022          | 0.4976           |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHCbLFja-JIt"
      },
      "source": [
        "X_train, y_train = train_bal['review'], train_bal['rating']\n",
        "X_test = test_df['review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYYjNRuP-JIw"
      },
      "source": [
        "import string\n",
        "def process_text(text):\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    return [word for word in word_tokenize(nopunc,'ulmfit') if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
        "def split_text(text):\n",
        "    return text.split()\n",
        "\n",
        "train_splits = []\n",
        "test_splits = []\n",
        "for i in tqdm_notebook(range(train_bal.shape[0])):\n",
        "    train_splits.append(' '.join(process_text(train_bal['review'][i])))\n",
        "for i in tqdm_notebook(range(test_df.shape[0])):\n",
        "    test_splits.append(' '.join(process_text(test_df['review'][i])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzHnBtFr-JIz"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=split_text, ngram_range=(1,2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa9yLDPH-JI2"
      },
      "source": [
        "pred_lab = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2S-ySq-JI5"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "pred_lab = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYY9CBXj-JI7"
      },
      "source": [
        "submit_df = pd.DataFrame({'reviewID':test_df.reviewID,\n",
        "                          'rating':pred_lab})\n",
        "submit_df.head()\n",
        "submit_df.to_csv('submit_linearsvc.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HgTabDr-JI9"
      },
      "source": [
        "## [ULMFit](https://github.com/cstorm125/thai2fit) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy-QShzM-JI-"
      },
      "source": [
        "| model     | micro_f1_public | micro_f1_private | \n",
        "|-----------|-----------------|------------------|\n",
        "| **ULMFit** | **0.59313**          | **0.60322**           |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta1kJIrr-JI-"
      },
      "source": [
        "# #uncomment if you are running from google colab\n",
        "# !pip install sklearn_crfsuite\n",
        "# !pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\n",
        "# !pip install fastai==1.0.45\n",
        "# !wget https://github.com/wongnai/wongnai-corpus/raw/master/review/review_dataset.zip; unzip review_dataset.zip\n",
        "# !mkdir wongnai_data; ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMKBC9Sr-JJA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "#viz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import CSVLogger\n",
        "\n",
        "from pythainlp.ulmfit import *\n",
        "\n",
        "model_path = 'wongnai_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX5XsaBi-JJE"
      },
      "source": [
        "#process data\n",
        "train_df = pd.read_csv('w_review_train.csv',sep=';',header=None).drop_duplicates()\n",
        "train_df.columns = ['review','rating']\n",
        "test_df = pd.read_csv('test_file.csv',sep=';')\n",
        "test_df['rating'] = 0\n",
        "all_df = pd.concat([pd.DataFrame(test_df['review']),\\\n",
        "                   pd.DataFrame(train_df['review'])]).reset_index(drop=True)\n",
        "two_df = pd.concat([train_df[train_df.rating==2].copy() for i in range(2)]).reset_index(drop=True)\n",
        "one_df = pd.concat([train_df[train_df.rating==1].copy() for i in range(10)]).reset_index(drop=True)\n",
        "train_bal = pd.concat([train_df,one_df,two_df]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqg6gWgd-JJG"
      },
      "source": [
        "### Finetune Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFuPt0VA-JJG"
      },
      "source": [
        "# tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th)\n",
        "# processor = [TokenizeProcessor(tokenizer=tt, chunksize=10000, mark_fields=False),\n",
        "#             NumericalizeProcessor(vocab=None, max_vocab=60000, min_freq=3)]\n",
        "\n",
        "# data_lm = (TextList.from_df(all_df, model_path, cols=['review'], processor=processor)\n",
        "#     .random_split_by_pct(valid_pct = 0.01, seed = 1412)\n",
        "#     .label_for_lm()\n",
        "#     .databunch(bs=64))\n",
        "# data_lm.sanity_check()\n",
        "# data_lm.save('wongnai_lm.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj6v9yP2-JJJ"
      },
      "source": [
        "data_lm = load_data(model_path,'wongnai_lm.pkl')\n",
        "data_lm.sanity_check()\n",
        "len(data_lm.train_ds), len(data_lm.valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG92Lvsa-JJL"
      },
      "source": [
        "data_lm.show_batch(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjpKP_TF-JJP"
      },
      "source": [
        "next(iter(data_lm.train_dl))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB13iHjX-JJR"
      },
      "source": [
        "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n",
        "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
        "trn_args = dict(drop_mult=0.9, clip=0.12, alpha=2, beta=1)\n",
        "\n",
        "learn = language_model_learner(data_lm, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
        "\n",
        "#load pretrained models\n",
        "learn.load_pretrained(**_THWIKI_LSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtqlA3vA-JJU"
      },
      "source": [
        "learn.predict('สวัสดีครับพี่น้องเสื้อ', 50, temperature=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWA_VLP6-JJW"
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP4pqvbj-JJZ"
      },
      "source": [
        "len(learn.data.vocab.itos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXWbw901-JJb"
      },
      "source": [
        "#train frozen\n",
        "print('training frozen')\n",
        "learn.freeze_to(-1)\n",
        "learn.fit_one_cycle(1, 1e-3, moms=(0.8, 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVwSiTHj-JJd"
      },
      "source": [
        "#train unfrozen\n",
        "print('training unfrozen')\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 1e-4, moms=(0.8, 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KgLDJ3R-JJf"
      },
      "source": [
        "learn.save('wongnai_lm')\n",
        "learn.save_encoder('wongnai_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdm6WsUw-JJi"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mLvuB37-JJi"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4gEdXVD-JJl"
      },
      "source": [
        "tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th)\n",
        "processor = [TokenizeProcessor(tokenizer=tt, chunksize=10000, mark_fields=False),\n",
        "            NumericalizeProcessor(vocab=data_lm.vocab, max_vocab=60000, min_freq=3)]\n",
        "\n",
        "data_cls = (TextList.from_df(train_bal, model_path, cols=['review'], processor=processor)\n",
        "    .random_split_by_pct(valid_pct = 0.01, seed = 1412)\n",
        "    .label_from_df('rating')\n",
        "    .add_test(TextList.from_df(test_df, model_path, cols=['review'], processor=processor))\n",
        "    .databunch(bs=32)\n",
        "    )\n",
        "\n",
        "data_cls.sanity_check()\n",
        "data_cls.save('wongnai_cls.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaxTHDj0-JJn"
      },
      "source": [
        "#make sure we got the right number of vocab\n",
        "len(data_cls.vocab.itos), len(data_lm.vocab.itos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz8ZVqeh-JJq"
      },
      "source": [
        "data_cls.show_batch(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foIe0LbG-JJs"
      },
      "source": [
        "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False,\n",
        "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
        "trn_args = dict(bptt=70, drop_mult=0.5, alpha=2, beta=1)\n",
        "\n",
        "learn = text_classifier_learner(data_cls, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
        "learn.opt_func = partial(optim.Adam, betas=(0.7, 0.99))\n",
        "\n",
        "#load pretrained finetuned model\n",
        "learn.load_encoder('wongnai_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9BRLXaP-JJu"
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufB2ICeM-JJx"
      },
      "source": [
        "# #train unfrozen\n",
        "# learn.freeze_to(-1)\n",
        "# learn.fit_one_cycle(1, 2e-2, moms=(0.8, 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU0H3u16-JJ1"
      },
      "source": [
        "# #gradual unfreezing\n",
        "# learn.freeze_to(-2)\n",
        "# learn.fit_one_cycle(1, slice(1e-2 / (2.6 ** 4), 1e-2), moms=(0.8, 0.7))\n",
        "# learn.freeze_to(-3)\n",
        "# learn.fit_one_cycle(1, slice(5e-3 / (2.6 ** 4), 5e-3), moms=(0.8, 0.7))\n",
        "# learn.unfreeze()\n",
        "# learn.fit_one_cycle(1, slice(1e-3 / (2.6 ** 4), 1e-3), moms=(0.8, 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNeAwyk4-JJ2"
      },
      "source": [
        "```\n",
        "epoch     train_loss  valid_loss  accuracy\n",
        "1         1.167613    1.109780    0.479079\n",
        "Total time: 08:22\n",
        "epoch     train_loss  valid_loss  accuracy\n",
        "1         0.982858    0.979201    0.560669\n",
        "2         0.870348    0.834990    0.598326\n",
        "3         0.752523    0.802491    0.629707\n",
        "4         0.653818    0.715869    0.671548\n",
        "5         0.559333    0.702696    0.682008\n",
        "Total time: 46:22\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8T55-3D-JJ3"
      },
      "source": [
        "### Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHgjFuwh-JJ3"
      },
      "source": [
        "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False,\n",
        "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
        "trn_args = dict(bptt=70, drop_mult=0.5, alpha=2, beta=1)\n",
        "\n",
        "learn = text_classifier_learner(data_cls, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
        "learn.opt_func = partial(optim.Adam, betas=(0.7, 0.99))\n",
        "\n",
        "learn.load('wongnai_cls');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jA498DV-JJ6"
      },
      "source": [
        "probs,y= learn.get_preds(DatasetType.Test, ordered=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC7XsNau-JJ8"
      },
      "source": [
        "preds = np.argmax(probs.numpy(),1) + 1\n",
        "Counter(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKwIxovP-JKA"
      },
      "source": [
        "submit_df = pd.DataFrame({'reviewID': test_df.reviewID,'rating':preds})\n",
        "submit_df.head()\n",
        "submit_df.to_csv('submit_ulmfit.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6cS5_a-9n2"
      },
      "source": [
        "## [Multilingual Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5GIYbJO_F5C"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import tensorflow as tf #tensorflow 2.1.0\n",
        "import tqdm\n",
        "\n",
        "enc = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Upc1GJCANr1"
      },
      "source": [
        "y_test, y_train = test_df['rating'], train_bal['rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGS5-x5LAAid"
      },
      "source": [
        "X_trains = []\n",
        "X_tests = []\n",
        "bs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5oUU4gwAAoU"
      },
      "source": [
        "for i in tqdm.tqdm_notebook(range(y_test.shape[0]//bs+1)):\n",
        "    X_tests.append(enc(test_df.review[(i*bs):((i+1)*bs)]).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiPz6zmnAAlZ"
      },
      "source": [
        "for i in tqdm.tqdm_notebook(range(y_train.shape[0]//bs+1)):\n",
        "    X_trains.append(enc(train_bal.review[(i*bs):((i+1)*bs)]).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABMb6ire_aec"
      },
      "source": [
        "X_test = np.concatenate(X_tests,0)\n",
        "X_train = np.concatenate(X_trains,0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7floxlb_LAu"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf = LinearSVC(class_weight='balanced')\n",
        "text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beOo_4hJA2O1"
      },
      "source": [
        "pred_lab = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvyOmBsJBDts"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe_enc = OneHotEncoder(handle_unknown='ignore')\n",
        "pred_lab = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nArapPBDA6Mw"
      },
      "source": [
        "submit_df = pd.DataFrame({'reviewID':test_df.reviewID,\n",
        "                          'rating':pred_lab})\n",
        "submit_df.head()\n",
        "submit_df.to_csv('submit_use.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgtaHPL7BXrs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}